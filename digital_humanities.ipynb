{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open ('sampledata.jsonl', 'r') as f:\n",
    "    json_list = list(f)\n",
    "    \n",
    "print (len (json_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's focus on just the first article in the collection\n",
    "\n",
    "article1 = json.loads(json_list[0])\n",
    "print (article1.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "articles = []\n",
    "for line in open('sampledata.jsonl', 'r'):\n",
    "    articles.append(json.loads(line))\n",
    " \n",
    "# data = dict(articles)\n",
    "# for element in data:\n",
    "#     if 'unigramCount' in element:\n",
    "#         del element['unigramCount']\n",
    "#     if 'bigramCount' in element:\n",
    "#         del element['bigramCount']\n",
    "#     if 'trigramCount' in element:\n",
    "#         del element['trigramCount']\n",
    "\n",
    "# with open('newdata.jsonl', 'w') as data_file:\n",
    "#     data = json.dump(data, data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here's a glance forward, at how we will loop through all the files\n",
    "\n",
    "for json_str in json_list:\n",
    "    article = json.loads(json_str)\n",
    "    print (article.keys())\n",
    "    print (\"Author: \" + str(article['creator'][0]), \" Title: \" + str(article['title']) )\n",
    "    #the zero after creator means 'just the first author in the list'\n",
    "    for key in article.keys():\n",
    "        print (key, article[key], \"\\n\")\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the error message: KeyError: 'creator'\n",
    "This means that some articles don't have an author name! This can happen in other fields as well. This kind of problem can really stump you. Here's a simple solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle errors without crashing--but this is really long and should not be run in the notebook. Too much printing text so I put in a break\n",
    "counter = 0\n",
    "for json_str in json_list:\n",
    "    article = json.loads(json_str)\n",
    "    print (article.keys())\n",
    "\n",
    "    try:\n",
    "        author = article['creator'][0]  \n",
    "    except:\n",
    "        author = \"No Author\"\n",
    "        \n",
    "    print (\"Author: \" + str(author), \" Title: \" + str(article['title']) )\n",
    "\n",
    "    \n",
    "    for key in article.keys():\n",
    "        print (key, article[key], \"\\n\")\n",
    "    \n",
    "    counter +=1\n",
    "    if counter > 10:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do these \"try\" \"except\" solutions to provide contents to categories that are sometimes missing, and thereby provide your program with stable data. Other categories besides \"author\" are sometimes missing as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we usually want to load all our data into the memory \n",
    "# Let's start by making a list of all the authors in the first 10 articles\n",
    "\n",
    "authors = []  #make a list of authors\n",
    "titles = [] #and titles\n",
    "counter = 0\n",
    "for json_str in json_list:\n",
    "    article = json.loads(json_str)\n",
    "\n",
    "    try:\n",
    "        authors.append(article['creator'][0]) #for simplicity, just take the first author\n",
    "    except:\n",
    "        na = \"No Author\"\n",
    "        authors.append(na)\n",
    "        \n",
    "    if counter <10:\n",
    "        print (authors)\n",
    "        counter +=1\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we redo the program without limitations. and add dates\n",
    "# let's also get the titles, dates and text\n",
    "\n",
    "authors = []  #make a list of authors\n",
    "titles = [] #and titles\n",
    "texts = [] #and fulltext\n",
    "dates = []\n",
    "\n",
    "for json_str in json_list:\n",
    "    article = json.loads(json_str)\n",
    "\n",
    "    try:\n",
    "        dates.append(article['datePublished'])\n",
    "    except:\n",
    "        dates.append(\"No Date\")\n",
    "    try:\n",
    "        authors.append(article['creator'][0]) #for simplicity, just take the first author\n",
    "    except:\n",
    "        na = \"No Author\"\n",
    "        authors.append(na)\n",
    "    \n",
    "    try:\n",
    "        titles.append(article['title'])\n",
    "    except:\n",
    "        t = \"No Title\"\n",
    "        titles.append(t)\n",
    "    \n",
    "    try:\n",
    "        texts.append(article['fullText'])\n",
    "    except:\n",
    "        t = \"No Text Available\"\n",
    "        texts.append(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took about ten seconds to read through 1500 files and make four lists in the memory. Now let's put these lists together into a sort of 'excel spreadsheet' called a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(list(zip(dates, authors, titles, texts)), \n",
    "                        columns = ['dates', 'authors', 'titles', 'texts'])\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now you might be wondering what that 'list(zip)' thing is. It has to do with the way that dataframes understand the list objects that we have created, and that we want to put into a sort of spreadsheet.\n",
    "To learn more about this, just google 'python dataframe list zip' and you'll find lots of resources. Stackexchange is usually the most useful.\n",
    "You can also use Copilot, which is an AI powered programming partner available in VS Code, and helps you to search for the right way to do things in your code. But those are topics that are too advanced for this crash course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail() #see the end of the dataframe as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's look at the text contained in the 'fulltext' field. \n",
    "\n",
    "print (df['texts'][0])  # I'm saying: show me the contents of the first row [0]\n",
    "                        # of the column titled 'texts' ['texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df['texts'][1]) #show me the second row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looks awful. Let's get rid of those ][ and \\n characters at least\n",
    "# first of all, what kind of variable is it anyway?\n",
    "\n",
    "text = df['texts'][0]\n",
    "type(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so the variable is actually saved as a list. Let's turn it into a string first of all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = str(text)\n",
    "type(text)\n",
    "print (text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So even though we turned it into a string, it still has those list characters at the beginning and end. Here's how we get rid of them: by 'slicing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text[2:-2]\n",
    "print (text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's get rid of the newline character \\n, and other useless whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.strip()\n",
    "print (text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This only got rid of the newlines that were not connected to words though! Now to get rid of the remaining newline characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \" \".join(text.split()) #this is what lots of online tutorials will tell you, but it doesn't work here. \n",
    "print (text2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = text2.replace(r'\\n', '') #sometimes you need to do a 'raw string literal' to get rid of pesky nasty useless characters\n",
    "print (text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get rid of one more thing, \\uf076\n",
    "\n",
    "text4 = text3.replace(r'\\uf076', '')\n",
    "print (text4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, you can see that we will be in the weeds for a while cleaning our text. This is a whole sub-section of DH. You will need to get used to googling 'python remove newline from string' and an arcane subject called regex.\n",
    "For our purposes, our text is now clean enough to start working on it. \n",
    "\n",
    "Please note! The quality of your output depends greatly on the quality of your input. Make sure you clean carefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's recreate our dataframe, with clean texts this time \n",
    "\n",
    "authors = []  #make a list of authors\n",
    "titles = [] #and titles\n",
    "texts = [] #and fulltext\n",
    "dates = []\n",
    "\n",
    "for json_str in json_list:\n",
    "    article = json.loads(json_str)\n",
    "    try:\n",
    "        dates.append(article['datePublished'])\n",
    "    except:\n",
    "        dates.append(\"No Date\")\n",
    "\n",
    "    try:\n",
    "        authors.append(article['creator'][0]) #for simplicity, just take the first author\n",
    "    except:\n",
    "        na = \"No Author\"\n",
    "        authors.append(na)\n",
    "    \n",
    "    try:\n",
    "        titles.append(article['title'])\n",
    "    except:\n",
    "        t = \"No Title\"\n",
    "        titles.append(t)\n",
    "    \n",
    "    try:\n",
    "        text = str(article['fullText']) #don't forget to make the list a string\n",
    "        text = text[2:-2] #cut off the first and last two characters\n",
    "        # text = text.strip() #get rid of extra whitespace\n",
    "        text = text.replace(r'\\n', '') #get rid of string literal \\n\n",
    "        text = text.replace(r'\\uf076', '') #eliminate whatever that is\n",
    "        texts.append(text) #stick the clean text into the list \n",
    "    except:\n",
    "        t = \"No Text Available\"\n",
    "        texts.append(t)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(list(zip(dates, authors, titles, texts)), \n",
    "                        columns = ['dates', 'authors', 'titles', 'texts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what can we do with this?\n",
    "Let's look for all the articles that have a keyword in their title, for instance 'India'.\n",
    "We can do this with our lists most easily (for simple applications). In more complicated situations we will use the dataframe, but we pay a price in terms of speed (dataframes are slower than lists) and for some operations this will be important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in titles:\n",
    "    if \"Women\" in t:\n",
    "        print (t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also find the index value, and use that to recover the author and title\n",
    "index_list = []\n",
    "for index, t in enumerate(titles): # the variable named 't' is the element of the list of titles, 'index' is its position in the list\n",
    "    if \"Women\" in t:\n",
    "        print (index, \"Author: \", authors[index], \"Title: \", t)\n",
    "        index_list.append(index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we need to make a choice about which direction we are going to go. I think one of the most exciting possibilities today is to use large language models like GPT-3 to automatically analyze our texts. But there are tons of other things we could do too! Some of them were presented by prof. Kulic. \n",
    "\n",
    "We're going to use a package called SpaCy for language modeling\n",
    "\n",
    "we need to pip install spacy\n",
    "\n",
    "then we go to https://spacy.io/usage and download our model\n",
    "\n",
    "python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "# import en_core_web_md\n",
    "\n",
    "doc = nlp(\"This is a sentence about red apples and green pears.\")\n",
    "print ([(w.text, w.pos_) for w in doc])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can break up a sentence into all its parts of speech, automatically!  Now let's see what else we can do with the tokenization of the sentence. Lots of information here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.dep_, token.is_stop)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the last article from the list above, index number 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df.iloc[20][3])  # get text from the dataframe, row 1196 and fourth column (count starting from zero)\n",
    "\n",
    "#we have a problem with the quality of the text--some sentences are running together, and the keywords and abstract are \n",
    "#all thrown together in here. But let's continue anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(df.iloc[20][3])\n",
    "\n",
    "for sentence in doc.sents:  #let's see how good a job it can do separating sentences out of the box\n",
    "    print (sentence, \"///////////////\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like it's doing ok. Let's proceed!\n",
    "adjectives = []  #initialize a list\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    for token in sentence:\n",
    "        # print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
    "        if token.is_stop: #skip over stop words\n",
    "            continue\n",
    "        if token.pos_ == 'ADJ':\n",
    "            print (token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can get a better list if we use lemmas, and filter for unique entries\n",
    "\n",
    "adjectives = []  #initialize a list\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    for token in sentence:\n",
    "        # print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
    "        if token.is_stop: #skip over stop words\n",
    "            continue\n",
    "        if token.pos_ == 'ADJ':\n",
    "            adjectives.append(token.lemma_)\n",
    "print (adjectives)  #print all the adjectives\n",
    "# filtered_list = set(adjectives) #get just the unique values from the list\n",
    "# print (filtered_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also count the duplicate elements, sort them in ascending order, and see which are most numerous\n",
    "\n",
    "my_dict = {i:adjectives.count(i) for i in adjectives}\n",
    "new_dict = dict(sorted(my_dict.items(), key=lambda item: item[1])) #some details here that we don't understand, we just believe...\n",
    "print (new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's do that for all the articles that had \"Imperial\" in their titles\n",
    "adjectives = []  #initialize a list\n",
    "\n",
    "for index in index_list:\n",
    "    doc = nlp(df.iloc[index][3])\n",
    "    for sentence in doc.sents:\n",
    "        for token in sentence:\n",
    "            # print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
    "            if token.is_stop: #skip over stop words\n",
    "                continue\n",
    "            if token.pos_ == 'ADJ':\n",
    "                adjectives.append(token.lemma_)\n",
    "                \n",
    "my_dict = {i:adjectives.count(i) for i in adjectives}\n",
    "\n",
    "new_dict = dict(sorted(my_dict.items(), key=lambda item: item[1])) #some details here that we don't understand, we just believe...\n",
    "print (new_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also do Named Entity Recognition, or NER, and find kinds of information in the articles that isn't a part of speech, but a more complex concept. For example, we can find all the named entities in the article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "d = \"When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.\"\n",
    "\n",
    "dd = nlp(d)\n",
    "displacy.render(dd, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's do that with our entire article 20\n",
    "\n",
    "d = nlp(df.iloc[20][3])\n",
    "\n",
    "displacy.render(d, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we can extract all the countries and people named in the article\n",
    "countries = []\n",
    "persons = []\n",
    "\n",
    "for entity in d.ents:\n",
    "    if entity.label_ == 'GPE':\n",
    "        countries.append(entity)\n",
    "    if entity.label_ == 'PERSON':\n",
    "        persons.append(entity)\n",
    "print (\"Countries: \", countries, \"\\n\")\n",
    "print (\"Persons: \", persons)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the list isn't perfect, we can see several errors. But in DH the point is usually to be able to ingest a large amount of information, more than you could read, and do analysis on that quantity. We expect to get it wrong sometimes (but we must minimize our errors!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one of the exciting aspects of LLMs is vectorization. We translate a text into a 'word vector' which can then be \n",
    "# manipulated in various ways. Here's an example.\n",
    "\n",
    "doc0 = nlp(\"Hindus worship many gods.\")\n",
    "doc1 = nlp(\"Judaism introduced monotheism to the near East.\")\n",
    "doc2 = nlp(\"I like burgers and fries.\")\n",
    "doc3 = nlp(\"Mesopotamian religion is polytheistic.\")\n",
    "\n",
    "# Similarity of two documents\n",
    "print(doc0, \"<->\", doc3, doc0.similarity(doc3))\n",
    "\n",
    "print(doc1, \"<->\", doc3, doc1.similarity(doc3))\n",
    "\n",
    "print(doc2, \"<->\", doc3, doc2.similarity(doc3))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the concept of similarity is complex. Do you think you could assign a percentage value to the similarity between any of the phrases we just saw? So let's take it with a grain of salt: the numbers coincide with what we would expect, but we shouldn't depend too much on just one number to characterize our texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Correlation remains among the most crucial concepts undergirding nearly every aspect of existing AI systems. According to Chun, correlation is not just a conceptual category, but it constitutes an everyday practice whereby people are lumped into “categories based on their being ‘like’ one another amplifying the effects of historical inequalities” [Chun 2021, 58]. These inequalities are in turn naturalized with data organization systems making it appear as though they are innate or sui generis categories which already preexist in the world. As Chun warns, “correlation contains within it the seeds of manipulation, segregation and misrepresentation” [Chun 2021, 59]. As a result of their reliance on correlation, social networks create “microidentities” by default which instrumentalize and weaponize individual differences. Data analytics consequently reimagines eugenics discourses within a big data future where correlations are not only assumed to be predictive of future outcomes, but surveillance is assumed to be a necessary component of every human institution and one which will allow humanity to improve nearly every component of daily life.\" http://www.digitalhumanities.org/dhq/vol/16/4/000656/000656.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's look at the most common words in the corpus\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#make a long string comprising every text in df.iloc[:,2]\n",
    "long_string = ' '.join(list(df.iloc[:,2].values))\n",
    "\n",
    "print (len (long_string)) #87838911 characters! This is too long to parse in one go, so we'll split it into chunks\n",
    "\n",
    "#split the long string into chunks of 1000000 characters\n",
    "chunks = [long_string[i:i+1000000] for i in range(0, len(long_string), 1000000)]\n",
    "\n",
    "#now we'll parse the first chunk and create a list of the most common words\n",
    "\n",
    "doc = nlp(chunks[0])\n",
    "\n",
    "# Create a list of word tokens\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "# Create a list of word tokens after removing punctuation\n",
    "punctuations = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "word_tokens = [word for word in tokens if word not in punctuations]\n",
    "\n",
    "# Create a list of word tokens after removing stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stopword_tokens = [word for word in word_tokens if not word in STOP_WORDS]\n",
    "\n",
    "# Create a list of word tokens after lemmatization\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "lemmas = [lemma for lemma in lemmas if lemma not in punctuations]\n",
    "lemmas = [lemma for lemma in lemmas if lemma not in STOP_WORDS]\n",
    "\n",
    "# Create a frequency list of tokens\n",
    "word_freq = Counter(lemmas)\n",
    "common_words = word_freq.most_common(10)\n",
    "print(common_words)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes a few seconds to calculate one hundred thousand characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's make a word cloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                min_font_size = 10).generate(long_string[:10000])  #shortening the string for speed\n",
    "\n",
    "# plot the WordCloud image\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "....word clouds, an absolutely unbearable and nearly useless object. Can we do anything more interesting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# we can do more advanced linguistic analysis with spaCy\n",
    "piano_text = \"Gus is learning to play piano\"\n",
    "piano_doc = nlp(piano_text)\n",
    "for token in piano_doc:\n",
    "    print(f\"\"\"TOKEN: {token.text}\n",
    "=====\n",
    "{token.tag_ = }\n",
    "{token.head.text = }\n",
    "{token.dep_ = }\"\"\"\n",
    ")\n",
    "\n",
    "displacy.render(piano_doc, style=\"dep\", jupyter=True) #this will open a browser window with a dependency graph of the sentence\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the basis of how I built a program to find the active and passive verbs related to divine figures in the Mesopotamian sources, in order to validate a claim about Mesopotamian gods being 'intransitive'. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making good-looking and informative visualizations is an art. It takes time and care, and often the best way to do it is to collaborate with an expert. I frequently work with data-visualization programmers, and pay a little to have a good visualization done quickly using my data backend. One of the best resources is D3, a visualization package you can learn about at https://d3js.org/ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
